Scheduler
    maximizing resources
        - utilization

    challenges
        - how to differentiate between IO vs CPU bound processes
        - limited information

    simflication assumptions
        - every job runs for the same amount of time
        - all jobs "arrive" at the same time
        - once scheduled, jobs run to completion
        - all jobs are CPU bound
        - runtimes for each job is known

    terms
        job = process
        workload = collection of running jobs
        non-preemptive = process isnt interrupted
        preemptive = process can be interrupted

    metrics of success
        - cpu utilization (idle time)
        - turnaround time (time between job arrival and job completion)
        - wait time (time spent ready but not running)
        - response time (time it takes for job to recieve user input/user interaction)
            - not always a clear computation
        - throughput (# jobs by time)

    users really only care about the appearance of performance though
        programs open quickly
        programs respond quickly

    Qualities of a process
        burst time          
        I/O requirements
        priority                - Kind of knows
        age                     - Knows
        other dependencies
        run time

    Scheduling Algorithms
        Batch scheduling
            first come first serve (FIFO queue)
                fair
                simple
                can be predictable
                convoy effect (1 super long job before 1000 short jobs; reduces throughput)
            shortest job first
                optimal for maximizing throughput
                non-preemptive
            shortest job next
                AKA shortest remaining job first
                AKA shortest remaining job next
                preemptive
                burst times compared at job arrival
                P1 @ 0 = 5, P2 @ 1 = 3, P3 @ 3 = 4
                ** make gantt chart and calculate wait time, turnaround time
            priority based scheduling
                taking age or some other metric into account when scheduling
            round robin scheduling
                everyone gets some amount of time on the cpu
                fcfs w/preemption based on quantum
                highly predictable
                low variance, potentially higher wait times
            multi-level feedback queueing
                create queues w/ different priorities
                1. if the priority(A) > priority(B), then A runs
                2. if priority(A) == priority(B), then A + B run using round robin
                3. when job arrives, placed in highest priority queue
                4a. if a job uses entire quantum, its priority is reduced
                4b. if job gives up the CPU before quantum is up (if job gets blocked)
                    it stays in current queue
                5. after some time S (age), move job to top-most queue

                    S is usually annecdotally derived; no way to figure it out perfectly

                stack:
                    
                    &A      <- points to previous base pointer (A)
                    &main
                    a
                    local
                    local
                    .
                    .
                    .
                    A

        Problems!:
            synchronization
                processes can be interrupted at any time
                threads can be interrupted at any time
                critical sections
                hogging critical sections due to happenstance

                any solution to the critical selection problem must have:
                    1) mutual exclusion (correctness)
                        only let one thread into critical section at a time
                        accomplished through lock
                    2) progress (fairness)
                        every thread will eventually enter critical section if it wants
                        cant let one thread continuously lock the critical section
                    3) bounded wait (performance)
                        limit the amount of time someone will wait for a lock to become available
                    cant rely on knowing the number of CPU's or speed of CPU's

        Solution 1: Peterson's solution
            software only solution
            for two threads (i, j) only

            // shared variables
            int turn
            bool flay[2]

            // thread i
            do{
                flag[i] = True
                turn = j
                while(flag[j] && turn == j); //wait
                
                // critical section
                
                flag[i] = false;
            } while(True)

        Hardware Solutions
            1) disable interrupts
                not very practical
                what if multiple CPU's?
            2) atomic instructions (does these things in 1 instruction)
                accomplishes by locking memory

                TestSetLock (lock)
                    bool TestAndSet(bool *lock){
                        bool ret = *lock;
                        *lock = TRUE;
                        return ret;
                    }

                    lock = FALSE;
                    do{
                        while(TestAndSet(&lock));   // this is called a spin lock
                        // CRITICAL SECTION
                        lock = FALSE;
                    } while(true)

                    does not solve 100%, only solves mutual exclusion; no progress/bounded wait

                    when to use spin lock? when critical section is small enough such that spin lock wait is
                    less expensive than context switching

                Swap
    
    Semaphores
        integer variable used for synchronization
            accessed through two atomic operations:

            wait(S){
                < insert lock here to achieve mutual exclusion? >
                S--;
                while(S < 0);
            }

            signal(S){
                S++;
            }

        Binary Semaphore = when S set to 1
            mutex lock
            do{
                wait(mutex);

                // critical section

                signal(mutex);
            } while (TRUE);

        can do synchronization
        can act as a barrier (limit number of threads allowed at a time)
            maximize core usage


        blocking semaphore
            semaphore S

            wait(S){
                S -> value--
                while(S->value < 0){
                    add_thread(S->queue)
                    sleep() <- magical thread sleep function
                }
            }

            signal(S){
                S->value++
                if (s->value <=0 ){
                    += get_thread(S->queue)
                    wakeup #
                }
            }

            // when thread waits, it gets added to a queue

            typedef struct_zem_t{
                int value
                pthread_cond_t cond
                pthread_mutex_t lock    

            }

            void zem-wait(zem_t *z){
                mutex_lock(&z->lock)    <- all of these functions are in the pthread library
                while(z->value <= 0)
                    cond_wait(&z->cond, &z->lock)   <- lock must reacquired before returning
                z->value --
                mutex_unlock(&z->lock)
            }

            void zen-post(zem_t *z){
                mutex_lock(&z->lock)
                z->value++
                cond_signal(&z->cond)
                mutex_unlock(&z->lock)
            }

        problems with locks/semaphores:
            T1:
                lock(S) 1
                lock(T) 3

            T2:
                lock(T) 2
                lock(S) 4

            T1, T2 need the variable locked by the opposite thread
            enter deadlock

        classic synchronization problems
            consumer/producer
            bounded buffer
            atomicity violations
                how threads access variables

                T1
                if (thd -> proc_info){
                    fputs(thd->proc_info...)
                    ...
                }

                T2
                if thd->proc_info = NULL

            ordering
                T1: (parent)
                    mThread = PIR_CreateThread(mMain, ...)
                
                T2:
                    void mMain(...){
                        // if this runs before assignment of mThread in T1, can cause issues
                        m State = mThread -> State
                    }

            dining philosopher's problem
                to eat, need 2 chopsticks
                to think, need 0 chopsticks
                if 5 philosophers and 5 chopsticks
                    how to maximize throughput

                
                void P(int i){
                    while(1){
                        think()

                        ** one solution = acquire two chopsticks at same time?
                            still have issue with release(), that also needs to be atomic
                            but if we make that atomic, then only 1 person can eat at a time
                            very subtle problems

                        wait(mutex)

                        acquire(chopstick[i])       // can get deadlock if all get preempted after this instructions
                        acquire(chopstick[i+1%N])

                        signal(mutex)
                        eat()

                        **
                        release(chopstick[i])
                        release(chopstick[i+i%N])
                    }
                }



                N = 5
                left (i+n-1) %n
                left (i+1) %n
                thinking 0
                hungry 1
                eating 2
                state[n]
                semaphore mutex = 1
                semaphore S[n]

                void p(int i){
                    while(1){
                        think()
                        take_chopsticks(i)
                        eat()
                        put_chopsticks(i)
                    }
                }

                take_chopsticks(int i){
                    wait(&mutex)
                    state[i] = hungry
                    test(i)
                    signal(&mutex)
                    wait(&s[i])
                }

                put_chopsticks(int i){
                    wait (&mutex)
                    state[i] = thinking
                    test(left)
                    test(right)
                    signal(mutex)
                }

                test(int i){
                    if(state[i] == hungry && 
                        state[left] != eating && 
                        state[right] != eating){
                            state[i] = eating
                            signal(&s[i])
                    }
                }

        deadlock
            a set of processes/threads are waiting for an event that only another process/thread in that set can cause

            resource deadlock
            communication deadlock
                A sends a message then wait()
                B sends something back then wait()
                works until a message gets blocked in transit

Deadlocks
    resource deadlocks

    ex:
        T1:             T2:
        lock(m1)        lock(m2)
        lock(m2)        lock(m1)

    encapsulation
        v1.addall(v2) == v2.addall(v1)
        except locks get called in a different order

    necessary conditions for deadlock
        mutual exclusion
            one resource held by one process at a time
        hold & wait
            holding one resource and waiting for another
        no resource preemption
            resource can only be released voluntarily
        circular wait
            a set of processes/threads {p0, p1, ... pn} such that Pi is waiting on resources used by P(i+1) % n

        cannot deadlock if all of these are not met

    how to handle deadlock:
        1. prevention: ensure deadlock can never occur
            break one of the conditions necessary for deadlock
        2. avoidance: deadlock is possible, but OS has info to avoid it
        3. detection: allow deadlock, but detect and recover from it
        4. Ostrich algorithm: head in the sand, pray
            do nothing

        most common = Ostrich
            Prevention is second most common but is often voluntary

    Prevention
        mutual exclusion
            get rid of locks/waiting
                can be done through a Monitor
                ex. printer queue
            lock/wait-free data structures
                ** atomic instruction
                int compare_and_swap(int *x, int expected, int new){
                    if (*x == expected){
                        *x = new;
                        return 1;
                    }
                    else{
                        return 0;
                    }
                }

                atomicIncrement(value, amount){
                    do{
                        int old = value;
                    }while(compare_and_swap(value, old, old+amount) == 0)
                }
        
        hold and wait
            a thread must request all resources at one time
            request resources only when it has none
            
            T1              T2
            lock(prevent)   lock(prevent)
            lock(m1)        lock(m2)
            lock(m2)        lock(m1)

            unlock(prevent) unlock(prevent)

            reduces concurrency;
        
        no preemption
            reclaiming locks is problematic
            ex stopping one job on 3d printer and giving control to another job
                BAD

            trylock()
                try acquire lock
                OR FAIL (instead of wait)

            T1              T2
            lock(m1)        lock(m2)
            trylock(m2)     trylock(m1)
            if(m2 == FAIL)  if(m1 == FAIL)
                unlock m1       unlock m2
                retry           retry

        circular wait
            impose a total ordering on how resources are acquired
            through policy or programmatic solution
            if (m1 > m2)
                acquire(m1)
                acquire(m2)
            else
                acquire(m2)
                acquire(m1)

    Avoidance
        (via scheduling)
        empower scheduler to schedule threads to avoid a dangerous or deadlock state

        dijkstra's banker's algorithm
            looks through matrix and sees if operation will result in deadlock state
            O(m x n^2) runtime for m resources and n processes
            run it every time a new process comes into the system
            EXPENSIVE
            could be important in critical hardware/systems

    Detection & Recovery
        wait-for graph and traversal can indicate deadlock
        heuristics
            cpu interaction
            process state
            allows user to decide how to handle deadlock state
        rollback
            send a process back into a non-deadlocked state

    typical solutions = write code following best practices and deal with a deadlock when it comes



File systems
    User Space
        < System Calls: Read, Write, Open/Close, Trunc

    Operating System
        Memory manager
        Virtual File System - sits above Disk File System
            where IO system calls are first serviced; handles initial call
            interface between all different types of file systems in the system
            provides consistency through different file systems
        Disk File System - executes IO system calls
            file + metadata gets organized into logical blocks for BDD
            forms: ext4, NTFS, AFS+, FAT32, etc.
            organizes data
            ** may be multiple Disk File Systems in a system; almost definitely are multiple
        Block Device Driver (BDD) - speaks to storage devices
            provides interface for logical blocks

    Storage Devices
        SSD/HDD

    
    Disk partitioning
        break hard drive into mutiple Disk File Systems
        first block = master boot record
            hold information to initialize file system
            have to know where the operating system is to start a program
            also has the partition table
                what are partitions on disk and which one holds the OS
            BIOS is told where MBR is which has Partition Table which has location of OS
        
        Disk File System Structure
            can vary greatly but commonalities between unix file systems

            | super block | free space management | INode Table | Root Directory | Data Blocks (Files-Directories-Links-etc.)

            super block
                contains file system-wide metadata
                    # of files
                    # of free/allocated blocks
                    FS type (AKA magic number) - decoder for magic number lives in fs.h 
                    time/access information
                    consistency (did this file system get unmounted cleanly or is it maybe broken?)

            free space management
                which blocks are allocated?
            
            INode
                per-file metadata
                every file has 1 INode
                stores location of the data blocks for given file
                access permissions
                ownership
                file sizes
                reference count - how many names point to this INode

            Directory(special file)
                contents = (name, INode #) tuples = directory entries (dirents)

            echo "hi!" > foo.txt
                block gets allocated for "hi!"
                inode gets allocated that points to said block
                (name, inode pointer) with (foo.txt, &inode) gets created in cwd

            Root Directory
                by convention is entry 2 in INode table

            Lookup operation
                find file starting at the root directory
                very expensive
                ex. /usr/home/znjp/foo.txt
                go to root directory
                read its INode
                read the data blocks
                look for usr/
                read its INode
                read the usr/ data blocks
                etc. etc. etc.

                every lookup has AT LEAST 2 IO operations
                    read inode then read inode's data blocks

                directory entry cache
                    take advantage of spacial locality
                    cache the INode of /usr/home/znjp/ b/c next operation will probably be something like
                    /usr/home/znjp/bar.txt
                    managed by virtual file system
                    makes it so only first lookup takes a long time


        Virtual File System
            provides:
                1. acts as switching layer for Disk File Systems
                2. uniform interface and namespace
                3. caching layer (in-memory FS objects)
            
            Example:
                /dev/hda1   # hard disk ** root fs
                /dev/sda1   # ssd
                /dev/cdrom  # CD
                /dev/fd0    # floppy disk

                mounting = initializing a device's file system
                mount /dev/fd0 /mnt/floppy
                virtual file system takes storage device and mapping into namespace
                    integrates different file systems
                ANY file can be configured to be a file system and then re-mounted to another location


    Disk File System Implementations
        needs:
        1. superblock
        2. INodes   (+ support for different file types)
        3. Data Blocks 

        metadata:
        clean/dirty data
        consistent/inconsistent data
        which objects allocated/unallocated

        considerations:
        device limitations/requirements/characteristics
        directory structures
        block indexing/multi-level indexing
            contiguous allocation for good sequential performance
                will get external fragmentation though
            linked allocation
                each block stores a pointer to next block
                limited to ONLY sequential access
                might not lead to good spatial locality (might not matter for SSD's)
            indexing
                INode
                | inode #
                | perm bits
                | time
                | index into logical HDD
                | i1
                | i2
                ...
                | in

                advantages: avoids external fragmentation (allocating blocks)
                    don't have to do sequential access
                    can use allocation strategies that keep data close together
                disadvantages: size of the inode can get pretty big and are limited in size
                    fix with multi-level indexing

Other file system features
    1. Continuous versioning (LFS)
    2. Snapshots
    3. content-based addressing
        hash contents to get an address into block space
        if multiple files have same contents, can share a block space
        dropbox did this for a time but then had security issues
    4. read-ahead
        read multiple blocks at once
        block level OR file level
        predictive pre-fetching of files that are often open together

Hard Disk Drives (HDD's)
    Disk Scheduling Algorithms
        fcfs
            simple, low-overhead
            fair
            sub-optimal with seeks
    Left-to-right Algorithms
        Scan
            goes from left of disk to right of disk back to left of disk
            can read/write both directions
        C-Scan
            only reads in 1 direction
            tries to even out wait times by giving edges the same priority
        Look (elevator algorithm)
            elevator doesn't travel the whole building
            only needs to travel as far as needed to service a request, then goes back
        C-look
    shortest seek time first
        need to know seek times though
        optimal in terms of response time
        may reach starvation

    all of the above are too elementary to implement in practice

    2D-Aware scheduling
    Shortest Access Time First
        more complicated seek curves
        cache-sensitive scheduling
    Age-Sensitive Scheduling
    Priority-based scheduling


Storage System Organizations
    Logical Volume Manager
        sits between Block Device Driver and Physical Storage Devices
        software translation layer that abstracts multiple physical disks into multiple logical disks
        allows for virtual volumes to be spread across multiple physical volumes
        allows for software layer between logical and physical blocks
            encryption
            Snapshots
            redundancy
            hot-swapping (if a drive fails, use a different drive)
            resizing
Redundant Array of Inexpensive Disks
    needs to solve:
        capacity
        performance
        redundancy

    failure rate:
        75000 hours per 1 failure
        if 1 million hard drives, 1 failure every .75 hours

    striping
        spreading data across multiple devices
    stripe
        size of allocation across all disks
    strip
        allocation on a single disk

    can only write whole stripes at a time
        stripe = multiple strips in a RAID
        can read/write to n disk strips at a time 
            where n is the number of disks in the RAID system
    
        increases capacity/throughput


    RAID 1: mirroring
        